{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 15:59:23.086141: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-08 15:59:23.594035: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-08 15:59:23.594045: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Flatten, GlobalMaxPooling2D\n",
    "import tensorflow.keras.applications.vgg16 as vgg16\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from yad2k.models.keras_yolo import yolo_head\n",
    "from yad2k.utils.utils import draw_boxes, get_colors_for_classes, scale_boxes, read_classes, read_anchors, preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('imdb.mat')\n",
    "data = data[\"imdb\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dob = data[0][0]\n",
    "photoTaken = data[1][0]\n",
    "fullPath = data[2][0]\n",
    "gender = data[3][0]\n",
    "name = data[4][0]\n",
    "faceLocation = data[5][0]  \n",
    "faceScore = data[6][0]\n",
    "secondFaceScore = data[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460723\n",
      "NUM OF TRAINING EXAMPLES 3657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 16:01:23.033932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-08 16:01:23.033945: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-08 16:01:23.033957: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (huwngnosleep): /proc/driver/nvidia/version does not exist\n",
      "2022-09-08 16:01:23.044001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# data_out = []\n",
    "X = []\n",
    "Y = []\n",
    "IMAGE_SIZE = 256\n",
    "print(len(dob))\n",
    "for i in range(len(dob)):\n",
    "    # test in dataset 01\n",
    "    if(fullPath[i][0][0: 2] != \"00\"): \n",
    "        continue\n",
    "        \n",
    "    image = {\n",
    "        \"dob\": dob[i],\n",
    "        \"fullPath\": fullPath[i],\n",
    "        \"faceLocation\": faceLocation[i],\n",
    "        \"faceScore\": faceScore[i]\n",
    "    }\n",
    "    try:\n",
    "        original_image, imageEncoded = preprocess_image(\"train_data/00/\" + image[\"fullPath\"][0], model_image_size = (IMAGE_SIZE, IMAGE_SIZE))    \n",
    "        width, height = original_image.size\n",
    "        left, top, right, bottom = image[\"faceLocation\"][0][0] / width, image[\"faceLocation\"][0][1] / height, image[\"faceLocation\"][0][2] / width, image[\"faceLocation\"][0][3] / height\n",
    "        if(np.array(imageEncoded[0]).shape == ((IMAGE_SIZE, IMAGE_SIZE))):\n",
    "            original_image.save(\"error_image.jpg\")\n",
    "            continue\n",
    "        X.append(imageEncoded[0])\n",
    "        Y.append((scipy.special.expit(image[\"faceScore\"]), left, top , right, bottom)) \n",
    "    except:\n",
    "        continue\n",
    "# X = np.array(X, dtype=object)\n",
    "# Y = np.array(Y, dtype=object)\n",
    "print(\"NUM OF TRAINING EXAMPLES\", len(X))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\n",
    "\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 512)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              1050624   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,775,557\n",
      "Trainable params: 1,060,869\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputLayer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "vgg_model = vgg16.VGG16(include_top=False)\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg = vgg_model(inputLayer)\n",
    "\n",
    "# f1 = GlobalMaxPooling2D()(vgg)\n",
    "# f1 = Dense(2048, activation='relu')(f1)\n",
    "# f1 = Dense(1, activation='sigmoid')(f1)\n",
    "\n",
    "f2 = GlobalMaxPooling2D()(vgg)\n",
    "f2 = Dense(2048, activation='relu')(f2)\n",
    "# f2 = Dense(4, activation='sigmoid')(f2)\n",
    "f2 = Dense(5, activation='sigmoid')(f2)\n",
    "\n",
    "# model = Model(inputs = inputLayer, outputs=[f1, f2])\n",
    "model = Model(inputs = inputLayer, outputs=f2)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def localizationLoss(yTrue, yHat):\n",
    "    deltaCoord = tf.reduce_sum(tf.square(yTrue[:,:2] - yHat[:,:2]))\n",
    "    hTrue = yTrue[:,3] - yTrue[:,1]\n",
    "    wTrue = yTrue[:,2] - yTrue[:,0]\n",
    "\n",
    "    hPred = yHat[:,3] - yHat[:,1]\n",
    "    wPred = yHat[:,2] - yHat[:,0]\n",
    "\n",
    "    deltaSize = tf.reduce_sum(tf.square(wTrue - wPred) + tf.square(hTrue - hPred))\n",
    "\n",
    "    return deltaCoord + deltaSize\n",
    "\n",
    "classificationLoss = tf.keras.losses.BinaryCrossentropy()\n",
    "def totalLoss(yTrue, yHat):\n",
    "    print(yTrue, yHat)\n",
    "    return classificationLoss(yTrue[0], yHat[0]) + localizationLoss(yTrue[1:], yHat[1:])\n",
    "opt = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetection(Model):\n",
    "    def __init__(self, model, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "    \n",
    "    def compile(self, opt, classLoss, localLoss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classLoss\n",
    "        self.lloss = localLoss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, Y = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)\n",
    "            print(\"out 1\", Y[0][0], classes, coords)\n",
    "            batch_closs = self.closs(Y[0], classes)\n",
    "            batch_lloss = self.lloss(tf.cast(Y[1:], tf.float32), coords)\n",
    "            print(batch_closs, batch_lloss)\n",
    "            total_loss = 0.5 * batch_closs + batch_lloss\n",
    "            print(\"total loss\", total_loss)\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "\n",
    "        self.opt.apply_gradients(zip(grad, total_loss, self.model.trainable_variables))\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_closs, \"regress_loss\": batch_lloss}\n",
    "    \n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FaceDetection(model)\n",
    "# model.compile(opt, classificationLoss, localizationLoss)\n",
    "# model.fit(train_dataset, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Tensor(\"Cast:0\", shape=(None, 5), dtype=float32) Tensor(\"model/dense_1/Sigmoid:0\", shape=(None, 5), dtype=float32)\n",
      "Tensor(\"Cast:0\", shape=(None, 5), dtype=float32) Tensor(\"model/dense_1/Sigmoid:0\", shape=(None, 5), dtype=float32)\n",
      "58/58 [==============================] - 221s 4s/step - loss: 43.4323\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 222s 4s/step - loss: 42.6455\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 221s 4s/step - loss: 62.5744\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 221s 4s/step - loss: 45.8374\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 220s 4s/step - loss: 45.9026\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 226s 4s/step - loss: 41.8201\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 226s 4s/step - loss: 37.6065\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 226s 4s/step - loss: 35.6224\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 229s 4s/step - loss: 35.5410\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 227s 4s/step - loss: 34.7994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4badda78e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=totalLoss)\n",
    "model.fit(train_dataset, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crop(imagePath, savedPath, left, top, right, bottom):\n",
    "    img = Image.open(imagePath) \n",
    "    img_res = img.crop((left, top, right, bottom)) \n",
    "    img_res.save(savedPath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 117ms/step\n",
      "1 68 50 162\n"
     ]
    }
   ],
   "source": [
    "model.save('savemodel.h5')\n",
    "test_image, test_image_data = preprocess_image(\"saved.jpeg\", model_image_size = (IMAGE_SIZE, IMAGE_SIZE))\n",
    "test_width, test_height = test_image.size\n",
    "result = model.predict(test_image_data)\n",
    "left, top, right, bottom = 1, int(result[0][2] * test_height), 100, int(result[0][4] * test_height)\n",
    "print(left, top, right, bottom)\n",
    "save_crop(\"saved.jpeg\", \"test_result.jpg\", left, top, right, bottom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
